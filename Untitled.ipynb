{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".....Loading model.....\n",
      "...Loading Images...\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torchvision import datasets,models,transforms\n",
    "import os\n",
    "from skimage import io \n",
    "from vismask import vismask \n",
    "\n",
    "outputimages = \"outputimages/img\"\n",
    "inputimages = \"inputimages/\"\n",
    "imgExt = \"JPEG\"\n",
    "\n",
    "imagenames = [fn for fn in os.listdir(inputimages) if fn.endswith(imgExt)]\n",
    "\n",
    "#Taking batches of 10 images of size 224x224\n",
    "imgCnt = 10\n",
    "imgCh = 3\n",
    "imgH = 224\n",
    "imgW = 224\n",
    "\n",
    "#Scaling and normalizing the images to required sizes (mean and std deviation are values required by trained VGG model)\n",
    "trans = transforms.Compose([transforms.ToPILImage(),\n",
    "\t\t\t\t\t\t\ttransforms.Scale(256),\n",
    "\t\t\t\t\t\t\ttransforms.CenterCrop(224),\n",
    "\t\t\t\t\t\t\ttransforms.ToTensor(),\n",
    "                            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
    "\n",
    "# imgBatch = torch.Tensor(imgCnt, imgCh, imgH, imgW).cuda()\n",
    "imgBatch = torch.Tensor(imgCnt, imgCh, imgH, imgW)\n",
    "\n",
    "#------------------------------------------------------------------------------------\n",
    "def getimages(n, out,fmaps,fmapsmasked):\n",
    "\n",
    "    h = out.size(2)\n",
    "    w = out.size(3)\n",
    "\n",
    "    scalingtr = nn.UpsamplingBilinear2d(size=(h,w))\n",
    "\n",
    "    imgout = torch.Tensor(3, imgH,imgW)\n",
    "\n",
    "    #placing all intermediate maps and masks in one big array\n",
    "    fMapsImg = torch.zeros(1,len(fmaps) * h + (len(fmaps) - 1) * 2, w)\n",
    "    fMapsImgM = torch.zeros(1,len(fmaps) * h + (len(fmaps) - 1) * 2, w)\n",
    "\n",
    "    for i in range(0,len(fmaps)):\n",
    "\n",
    "        #normalization\n",
    "        minvalue = fmaps[i][n,0].min()\n",
    "        maxvalue = fmaps[i][n,0].max()\n",
    "        fmaps[i][n] = torch.add(fmaps[i][n],-minvalue)\n",
    "        fmaps[i][n] = torch.div(fmaps[i][n],(maxvalue-minvalue))\n",
    "\n",
    "        #normalization\n",
    "        minvalue = fmapsmasked[i][n,0].min()\n",
    "        maxvalue = fmapsmasked[i][n,0].max()\n",
    "        fmapsmasked[i][n] = torch.add(fmapsmasked[i][n],-minvalue)\n",
    "        fmapsmasked[i][n] = torch.div(fmapsmasked[i][n],(maxvalue-minvalue))\n",
    "        \n",
    "        #saving the normalized map and mask\n",
    "        fMapsImg.narrow(1,(i)*(h+2),w).copy_(scalingtr(Variable(fmaps[i].float())).data[n])\n",
    "        fMapsImgM.narrow(1,(i)*(h+2),w).copy_(scalingtr(Variable(fmapsmasked[i].float())).data[n])\n",
    "\n",
    "    imgout[0].copy_(imgBatch[n][0].data).add(out[n][0])\n",
    "    imgout[1].copy_(imgBatch[n][0].data).add(-out[n][0])\n",
    "    imgout[2].copy_(imgBatch[n][0].data).add(-out[n][0])\n",
    "    imgout.clamp(0,1)\n",
    "    \n",
    "    return imgout,fMapsImg,fMapsImgM\n",
    "\n",
    "#------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "print (\".....Loading model.....\")\n",
    "# model = torch.load('model.pth')\n",
    "model = models.vgg16(pretrained=True)\n",
    "\n",
    "print (\"...Loading Images...\")\n",
    "\n",
    "for i in range (0,10):\n",
    "\timgBatch[i,:,:,:] = trans(io.imread(os.path.join(inputimages,imagenames[i])))\n",
    "\n",
    "imgBatch = Variable(imgBatch, volatile = True)\n",
    "\n",
    "#Obtain visualization mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VGG (\n",
       "  (features): Sequential (\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU (inplace)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU (inplace)\n",
       "    (4): MaxPool2d (size=(2, 2), stride=(2, 2), dilation=(1, 1))\n",
       "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU (inplace)\n",
       "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU (inplace)\n",
       "    (9): MaxPool2d (size=(2, 2), stride=(2, 2), dilation=(1, 1))\n",
       "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU (inplace)\n",
       "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU (inplace)\n",
       "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): ReLU (inplace)\n",
       "    (16): MaxPool2d (size=(2, 2), stride=(2, 2), dilation=(1, 1))\n",
       "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (18): ReLU (inplace)\n",
       "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (20): ReLU (inplace)\n",
       "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (22): ReLU (inplace)\n",
       "    (23): MaxPool2d (size=(2, 2), stride=(2, 2), dilation=(1, 1))\n",
       "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (25): ReLU (inplace)\n",
       "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (27): ReLU (inplace)\n",
       "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (29): ReLU (inplace)\n",
       "    (30): MaxPool2d (size=(2, 2), stride=(2, 2), dilation=(1, 1))\n",
       "  )\n",
       "  (classifier): Sequential (\n",
       "    (0): Linear (25088 -> 4096)\n",
       "    (1): ReLU (inplace)\n",
       "    (2): Dropout (p = 0.5)\n",
       "    (3): Linear (4096 -> 4096)\n",
       "    (4): ReLU (inplace)\n",
       "    (5): Dropout (p = 0.5)\n",
       "    (6): Linear (4096 -> 1000)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "A = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "A[13] = 'hello'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{13: 'hello'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A[13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = torch.Tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
    "y = torch.Tensor([10, 20, 30])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 10  20  30\n",
       "[torch.FloatTensor of size 1x3]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.narrow(0,0,1).copy_(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 10  20  30\n",
       "  4   5   6\n",
       "  7   8   9\n",
       "[torch.FloatTensor of size 3x3]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
